import asyncio
import logging
import time
import re
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from models import StudyChatRequest, StudyChatResponse

# Import Google Generative AI
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

class SubjectDetector:
    """Detect academic subjects from Vietnamese text"""
    
    def __init__(self):
        self.subject_keywords = {
            'to√°n': ['to√°n', 'to√°n h·ªçc', 'mathematics', 'math', 'ph∆∞∆°ng tr√¨nh', 'h√¨nh h·ªçc', 'ƒë·∫°i s·ªë', 'gi·∫£i t√≠ch', 's·ªë h·ªçc', 't√≠nh to√°n', 'c√¥ng th·ª©c', 'b√†i t·∫≠p to√°n', 'gi·∫£i to√°n'],
            'l√Ω': ['v·∫≠t l√Ω', 'physics', 'l·ª±c', 'ƒëi·ªán', 't·ª´', 'quang', 'c∆° h·ªçc', 'nhi·ªát', 's√≥ng', '√°nh s√°ng', 'electron', 'v·∫≠t l√≠', 'b√†i t·∫≠p l√Ω'],
            'h√≥a': ['h√≥a h·ªçc', 'chemistry', 'ph√¢n t·ª≠', 'nguy√™n t·ªë', 'ph·∫£n ·ª©ng', 'axit', 'baz∆°', 'ion', 'h·ª£p ch·∫•t', 'h√≥a l√Ω', 'b√†i t·∫≠p h√≥a'],
            'sinh': ['sinh h·ªçc', 'biology', 't·∫ø b√†o', 'gen', 'adn', 'protein', 'quang h·ª£p', 'sinh v·∫≠t', 'ƒë·ªông v·∫≠t', 'th·ª±c v·∫≠t', 'sinh l√Ω', 'b√†i t·∫≠p sinh'],
            'vƒÉn': ['vƒÉn h·ªçc', 'literature', 'th∆°', 'truy·ªán', 'ti·ªÉu thuy·∫øt', 't√°c ph·∫©m', 't√°c gi·∫£', 'ph√¢n t√≠ch', 'c·∫£m nh·∫≠n', 'vƒÉn b·∫£n', 'b√†i vƒÉn', 'lu·∫≠n vƒÉn'],
            's·ª≠': ['l·ªãch s·ª≠', 'history', 'chi·∫øn tranh', 'vua', 'tri·ªÅu ƒë·∫°i', 'c√°ch m·∫°ng', 's·ª± ki·ªán l·ªãch s·ª≠', 'th·ªùi ƒë·∫°i', 'b√†i s·ª≠'],
            'ƒë·ªãa': ['ƒë·ªãa l√Ω', 'geography', 'kh√≠ h·∫≠u', 'ƒë·ªãa h√¨nh', 'l·ª•c ƒë·ªãa', 'ƒë·∫°i d∆∞∆°ng', 'qu·ªëc gia', 'th·ªß ƒë√¥', 'b·∫£n ƒë·ªì', 'b√†i ƒë·ªãa'],
            'anh': ['ti·∫øng anh', 'english', 'grammar', 'vocabulary', 't·ª´ v·ª±ng', 'ng·ªØ ph√°p', 'speaking', 'listening', 'reading', 'writing', 'b√†i ti·∫øng anh'],
            'tin': ['tin h·ªçc', 'computer science', 'l·∫≠p tr√¨nh', 'programming', 'algorithm', 'thu·∫≠t to√°n', 'code', 'm√°y t√≠nh', 'b√†i tin h·ªçc'],
            'kinh t·∫ø': ['kinh t·∫ø', 'economics', 'th·ªã tr∆∞·ªùng', 'cung c·∫ßu', 'gdp', 'l·∫°m ph√°t', 'ƒë·∫ßu t∆∞', 't√†i ch√≠nh'],
            'kh√°c': ['h·ªçc t·∫≠p', 'gi√°o d·ª•c', 'b√†i t·∫≠p', 'ki·ªÉm tra', 'thi c·ª≠', 'nghi√™n c·ª©u', '√¥n t·∫≠p']
        }
    
    def detect_subject(self, text: str) -> str:
        """Detect subject from text"""
        text_lower = text.lower()
        
        subject_scores = {}
        for subject, keywords in self.subject_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in text_lower:
                    score += len(keyword)  # Longer keywords get higher scores
            
            if score > 0:
                subject_scores[subject] = score
        
        if subject_scores:
            return max(subject_scores.items(), key=lambda x: x[1])[0]
        
        return 'kh√°c'

class StudyAssistantLLM:
    """Study assistant using Google Gemini API for Vietnamese education"""
    
    def __init__(self, api_key: str = None):
        # ƒê·ªçc API key t·ª´ environment variables
        self.api_key = api_key or os.getenv('GOOGLE_API_KEY')
        self.model_name = os.getenv('GEMINI_MODEL_NAME', 'gemini-2.0-flash-exp')
        self.model = None
        self.is_loaded = False
        
        # Vietnamese education context
        self.education_context = {
            'grade_levels': ['ti·ªÉu h·ªçc', 'trung h·ªçc c∆° s·ªü', 'trung h·ªçc ph·ªï th√¥ng', 'ƒë·∫°i h·ªçc'],
            'common_questions': [
                'gi·∫£i th√≠ch', 'ph√¢n t√≠ch', 'so s√°nh', 'ƒë√°nh gi√°', 
                't√≠nh to√°n', 'ch·ª©ng minh', 'm√¥ t·∫£', 'k·ªÉ t√™n'
            ],
            'response_formats': {
                'explanation': 'Gi·∫£i th√≠ch chi ti·∫øt v·ªõi v√≠ d·ª• c·ª• th·ªÉ',
                'calculation': 'H∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc t√≠nh to√°n',
                'analysis': 'Ph√¢n t√≠ch ƒëa chi·ªÅu v·ªõi lu·∫≠n ƒëi·ªÉm r√µ r√†ng',
                'comparison': 'So s√°nh theo nhi·ªÅu ti√™u ch√≠ kh√°c nhau'
            }
        }
        
    async def load_model(self) -> bool:
        """Initialize Google Gemini API"""
        try:
            logger.info("Initializing Google Gemini API...")
            
            # Configure the API
            genai.configure(api_key=self.api_key)
            
            # Initialize the model
            self.model = genai.GenerativeModel(self.model_name)
            
            # Test the connection
            test_response = await self._generate_test_response()
            if test_response:
                self.is_loaded = True
                logger.info("Google Gemini API initialized successfully")
                return True
            else:
                logger.error("Failed to test Gemini API connection")
                return False
            
        except Exception as e:
            logger.error(f"Error initializing Gemini API: {e}")
            return False
    
    async def _generate_test_response(self) -> bool:
        """Test Gemini API connection"""
        try:
            response = self.model.generate_content("Hello, test connection")
            return response and response.text
        except Exception as e:
            logger.error(f"Gemini API test failed: {e}")
            return False
    
    def create_study_prompt(self, question: str, subject: str, difficulty: str = "intermediate") -> str:
        """Create specialized prompt for educational content"""
        
        # Base system prompt for Vietnamese education
        system_prompt = """B·∫°n l√† m·ªôt gi√°o vi√™n AI chuy√™n nghi·ªáp, gi·ªèi gi·∫£ng d·∫°y c√°c m√¥n h·ªçc ·ªü Vi·ªát Nam. 
B·∫°n c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ ch∆∞∆°ng tr√¨nh gi√°o d·ª•c Vi·ªát Nam v√† c√≥ th·ªÉ gi·∫£ng d·∫°y t·ª´ c·∫•p ti·ªÉu h·ªçc ƒë·∫øn ƒë·∫°i h·ªçc.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt, d·ªÖ hi·ªÉu v√† ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô h·ªçc sinh Vi·ªát Nam.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:
1. S·ª≠ d·ª•ng ti·∫øng Vi·ªát r√µ r√†ng, d·ªÖ hi·ªÉu
2. Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc m·ªôt c√°ch logic v√† c√≥ h·ªá th·ªëng
3. ƒê∆∞a ra v√≠ d·ª• c·ª• th·ªÉ, g·∫ßn g≈©i v·ªõi h·ªçc sinh Vi·ªát Nam
4. Khuy·∫øn kh√≠ch t∆∞ duy ph·∫£n bi·ªán v√† s√°ng t·∫°o
5. Li√™n h·ªá v·ªõi th·ª±c t·∫ø Vi·ªát Nam khi ph√π h·ª£p
6. S·ª≠ d·ª•ng emojis ƒë·ªÉ l√†m cho c√¢u tr·∫£ l·ªùi sinh ƒë·ªông h∆°n
7. Lu√¥n k·∫øt th√∫c b·∫±ng l·ªùi khuy·∫øn kh√≠ch h·ªçc t·∫≠p

"""
        
        # Subject-specific instructions
        subject_instructions = {
            'to√°n': '''üìä TO√ÅN H·ªåC: 
- Gi·∫£i th√≠ch c√°c b∆∞·ªõc t√≠nh to√°n chi ti·∫øt, r√µ r√†ng
- S·ª≠ d·ª•ng c√¥ng th·ª©c v√† v√≠ d·ª• s·ªë c·ª• th·ªÉ
- H∆∞·ªõng d·∫´n ph∆∞∆°ng ph√°p gi·∫£i t·ª´ng b∆∞·ªõc
- Ki·ªÉm tra k·∫øt qu·∫£ v√† gi·∫£i th√≠ch √Ω nghƒ©a''',
            
            'l√Ω': '''‚ö° V·∫¨T L√ù:
- M√¥ t·∫£ hi·ªán t∆∞·ª£ng v·∫≠t l√Ω m·ªôt c√°ch sinh ƒë·ªông
- Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông v√† ƒë·ªãnh lu·∫≠t
- ƒê∆∞a ra ·ª©ng d·ª•ng th·ª±c t·∫ø trong cu·ªôc s·ªëng
- S·ª≠ d·ª•ng s∆° ƒë·ªì, h√¨nh ·∫£nh minh h·ªça (m√¥ t·∫£ b·∫±ng text)''',
            
            'h√≥a': '''üß™ H√ìA H·ªåC:
- Gi·∫£i th√≠ch ph·∫£n ·ª©ng h√≥a h·ªçc v√† c∆° ch·∫ø
- M√¥ t·∫£ t√≠nh ch·∫•t c√°c ch·∫•t v√† ·ª©ng d·ª•ng
- Nh·∫•n m·∫°nh an to√†n h√≥a h·ªçc
- Li√™n h·ªá v·ªõi m√¥i tr∆∞·ªùng v√† cu·ªôc s·ªëng''',
            
            'sinh': '''üå± SINH H·ªåC:
- M√¥ t·∫£ c·∫•u tr√∫c v√† ch·ª©c nƒÉng sinh h·ªçc
- Gi·∫£i th√≠ch m·ªëi li√™n h·ªá trong h·ªá sinh th√°i
- Li√™n h·ªá v·ªõi s·ª©c kh·ªèe con ng∆∞·ªùi
- ƒê∆∞a ra v√≠ d·ª• v·ªÅ sinh v·∫≠t ·ªü Vi·ªát Nam''',
            
            'vƒÉn': '''üìö VƒÇN H·ªåC:
- Ph√¢n t√≠ch t√°c ph·∫©m vƒÉn h·ªçc m·ªôt c√°ch s√¢u s·∫Øc
- Gi·∫£i th√≠ch ngh·ªá thu·∫≠t ng√¥n t·ª´ v√† bi·ªán ph√°p tu t·ª´
- ƒê√°nh gi√° gi√° tr·ªã nh√¢n vƒÉn v√† x√£ h·ªôi
- Li√™n h·ªá v·ªõi vƒÉn h√≥a Vi·ªát Nam''',
            
            's·ª≠': '''üèõÔ∏è L·ªäCH S·ª¨:
- Tr√¨nh b√†y s·ª± ki·ªán l·ªãch s·ª≠ theo th·ª© t·ª± th·ªùi gian
- Ph√¢n t√≠ch nguy√™n nh√¢n - k·∫øt qu·∫£
- R√∫t ra b√†i h·ªçc kinh nghi·ªám
- T√¥n vinh nh·ªØng gi√° tr·ªã l·ªãch s·ª≠ Vi·ªát Nam''',
            
            'ƒë·ªãa': '''üåç ƒê·ªäA L√ù:
- M√¥ t·∫£ ƒë·ªãa l√Ω t·ª± nhi√™n v√† kinh t·∫ø-x√£ h·ªôi
- Gi·∫£i th√≠ch t∆∞∆°ng t√°c gi·ªØa con ng∆∞·ªùi v√† m√¥i tr∆∞·ªùng
- S·ª≠ d·ª•ng s·ªë li·ªáu c·ª• th·ªÉ v·ªÅ Vi·ªát Nam
- ƒê∆∞a ra nh·ªØng v·∫•n ƒë·ªÅ m√¥i tr∆∞·ªùng hi·ªán t·∫°i''',
            
            'anh': '''üá¨üáß TI·∫æNG ANH:
- Gi·∫£i th√≠ch ng·ªØ ph√°p m·ªôt c√°ch d·ªÖ hi·ªÉu
- Cung c·∫•p t·ª´ v·ª±ng phong ph√∫ v·ªõi v√≠ d·ª•
- H∆∞·ªõng d·∫´n c√°ch ph√°t √¢m ch√≠nh x√°c
- ƒê∆∞a ra t√¨nh hu·ªëng giao ti·∫øp th·ª±c t·∫ø''',
            
            'tin': '''üíª TIN H·ªåC:
- H∆∞·ªõng d·∫´n l·∫≠p tr√¨nh t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao
- Gi·∫£i th√≠ch thu·∫≠t to√°n v√† c·∫•u tr√∫c d·ªØ li·ªáu
- ƒê∆∞a ra v√≠ d·ª• m√£ ngu·ªìn c·ª• th·ªÉ
- ·ª®ng d·ª•ng c√¥ng ngh·ªá trong gi√°o d·ª•c''',
        }
        
        # Difficulty adjustments
        difficulty_instructions = {
            'beginner': '''üåü TR√åNH ƒê·ªò C∆† B·∫¢N:
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu
- V√≠ d·ª• g·∫ßn g≈©i v·ªõi ƒë·ªùi s·ªëng h·ªçc sinh
- Tr√°nh thu·∫≠t ng·ªØ ph·ª©c t·∫°p
- Gi·∫£i th√≠ch t·ª´ nh·ªØng ki·∫øn th·ª©c c∆° b·∫£n nh·∫•t''',
            
            'intermediate': '''üìà TR√åNH ƒê·ªò TRUNG B√åNH:
- C√¢n b·∫±ng gi·ªØa l√Ω thuy·∫øt v√† th·ª±c h√†nh
- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ chuy√™n m√¥n ph√π h·ª£p
- ƒê∆∞a ra nhi·ªÅu v√≠ d·ª• minh h·ªça
- K·∫øt n·ªëi ki·∫øn th·ª©c ƒë√£ h·ªçc''',
            
            'advanced': '''üéì TR√åNH ƒê·ªò N√ÇNG CAO:
- ƒêi s√¢u v√†o chi ti·∫øt v√† ph√¢n t√≠ch nhi·ªÅu g√≥c ƒë·ªô
- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ chuy√™n m√¥n ch√≠nh x√°c
- Th√°ch th·ª©c t∆∞ duy v·ªõi c√°c v·∫•n ƒë·ªÅ ph·ª©c t·∫°p
- Khuy·∫øn kh√≠ch nghi√™n c·ª©u s√¢u h∆°n'''
        }
        
        # Construct full prompt
        prompt = system_prompt
        
        if subject in subject_instructions:
            prompt += f"\n{subject_instructions[subject]}\n"
        
        if difficulty in difficulty_instructions:
            prompt += f"\n{difficulty_instructions[difficulty]}\n"
        
        prompt += f"""
üìù C√ÇU H·ªéI C·ª¶A H·ªåC SINH: {question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi n√†y m·ªôt c√°ch chi ti·∫øt, d·ªÖ hi·ªÉu v√† c√≥ t√≠nh khuy·∫øn kh√≠ch. 
ƒê·ª´ng qu√™n s·ª≠ d·ª•ng emojis ƒë·ªÉ l√†m cho c√¢u tr·∫£ l·ªùi sinh ƒë·ªông v√† th√∫ v·ªã h∆°n!
"""
        
        return prompt
    
    async def generate_response(self, question: str, subject: str = "kh√°c", difficulty: str = "intermediate") -> Tuple[str, float]:
        """Generate educational response using Gemini API"""
        if not self.is_loaded:
            return self._generate_fallback_response(question, subject), 0.5
        
        try:
            # Create educational prompt
            prompt = self.create_study_prompt(question, subject, difficulty)
            
            logger.info("Generating study assistance response with Gemini...")
            
            start_time = time.time()
            
            # Generate response using Gemini
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    max_output_tokens=1024,
                )
            )
            
            generation_time = time.time() - start_time
            
            if response and response.text:
                # Post-process response
                response_text = self._post_process_response(response.text)
                
                logger.info(f"Response generated in {generation_time:.2f}s")
                return response_text, 0.95
            else:
                logger.warning("Empty response from Gemini API")
                return self._generate_fallback_response(question, subject), 0.5
                
        except Exception as e:
            logger.error(f"Error in Gemini generation: {e}")
            return self._generate_fallback_response(question, subject), 0.3
    
    def _post_process_response(self, response: str) -> str:
        """Post-process the generated response"""
        # Remove any incomplete sentences at the end
        response = response.strip()
        
        # Clean up common generation artifacts
        response = re.sub(r'\n+', '\n', response)
        response = re.sub(r' +', ' ', response)
        
        # Ensure the response ends with proper punctuation
        if response and not response[-1] in '.!?':
            response += '.'
        
        return response.strip()
    
    def _generate_fallback_response(self, question: str, subject: str) -> str:
        """Generate fallback response when Gemini API is not available"""
        logger.info("Using fallback study response")
        
        # Subject-specific fallback responses
        fallback_responses = {
            'to√°n': f"""üìä C√¢u h·ªèi to√°n h·ªçc: "{question}"

üî¢ ƒê·ªÉ gi·∫£i quy·∫øt b√†i to√°n n√†y, t√¥i g·ª£i √Ω c√°c b∆∞·ªõc sau:

1. **X√°c ƒë·ªãnh d·ªØ li·ªáu**: ƒê·ªçc k·ªπ ƒë·ªÅ b√†i, x√°c ƒë·ªãnh nh·ªØng g√¨ ƒë√£ cho v√† c·∫ßn t√¨m
2. **Ch·ªçn ph∆∞∆°ng ph√°p**: X√°c ƒë·ªãnh c√¥ng th·ª©c ho·∫∑c ph∆∞∆°ng ph√°p gi·∫£i ph√π h·ª£p
3. **Th·ª±c hi·ªán t√≠nh to√°n**: √Åp d·ª•ng c√¥ng th·ª©c v√† t√≠nh to√°n t·ª´ng b∆∞·ªõc
4. **Ki·ªÉm tra k·∫øt qu·∫£**: Xem k·∫øt qu·∫£ c√≥ h·ª£p l√Ω kh√¥ng

üí° **L∆∞u √Ω**: H√£y ki·ªÉm tra l·∫°i ph√©p t√≠nh v√† ƒë∆°n v·ªã ƒëo l∆∞·ªùng!
üåü **Khuy·∫øn kh√≠ch**: To√°n h·ªçc l√† ng√¥n ng·ªØ c·ªßa v≈© tr·ª•, h√£y ki√™n tr√¨ luy·ªán t·∫≠p!""",

            'l√Ω': f"""‚ö° C√¢u h·ªèi v·∫≠t l√Ω: "{question}"

üî¨ ƒê·ªÉ hi·ªÉu r√µ hi·ªán t∆∞·ª£ng v·∫≠t l√Ω n√†y:

1. **Quan s√°t hi·ªán t∆∞·ª£ng**: M√¥ t·∫£ nh·ªØng g√¨ x·∫£y ra
2. **T√¨m nguy√™n l√Ω**: X√°c ƒë·ªãnh ƒë·ªãnh lu·∫≠t v·∫≠t l√Ω li√™n quan
3. **Ph√¢n t√≠ch nguy√™n nh√¢n**: Gi·∫£i th√≠ch t·∫°i sao hi·ªán t∆∞·ª£ng x·∫£y ra
4. **·ª®ng d·ª•ng th·ª±c t·∫ø**: T√¨m v√≠ d·ª• trong cu·ªôc s·ªëng

üî¨ **G·ª£i √Ω**: H√£y li√™n h·ªá v·ªõi nh·ªØng hi·ªán t∆∞·ª£ng t∆∞∆°ng t·ª± trong th·ª±c t·∫ø!
üåü **Khuy·∫øn kh√≠ch**: V·∫≠t l√Ω gi√∫p b·∫°n hi·ªÉu th·∫ø gi·ªõi xung quanh!""",

            'h√≥a': f"""üß™ C√¢u h·ªèi h√≥a h·ªçc: "{question}"

‚öóÔ∏è ƒê·ªÉ gi·∫£i quy·∫øt b√†i t·∫≠p h√≥a h·ªçc:

1. **X√°c ƒë·ªãnh ch·∫•t tham gia**: Vi·∫øt c√¥ng th·ª©c h√≥a h·ªçc c√°c ch·∫•t
2. **C√¢n b·∫±ng ph∆∞∆°ng tr√¨nh**: ƒê·∫£m b·∫£o s·ªë nguy√™n t·ª≠ m·ªói nguy√™n t·ªë b·∫±ng nhau
3. **T√≠nh to√°n theo t·ª∑ l·ªá**: S·ª≠ d·ª•ng s·ªë mol v√† kh·ªëi l∆∞·ª£ng mol
4. **Ki·ªÉm tra k·∫øt qu·∫£**: Xem c√≥ h·ª£p l√Ω v·ªÅ m·∫∑t h√≥a h·ªçc kh√¥ng

‚öóÔ∏è **An to√†n**: Lu√¥n ch√∫ √Ω ƒë·∫øn t√≠nh ch·∫•t v√† ƒë·ªô nguy hi·ªÉm c·ªßa c√°c ch·∫•t!
üåü **Khuy·∫øn kh√≠ch**: H√≥a h·ªçc l√† ch√¨a kh√≥a ƒë·ªÉ hi·ªÉu cu·ªôc s·ªëng!""",

            'sinh': f"""üå± C√¢u h·ªèi sinh h·ªçc: "{question}"

üî¨ ƒê·ªÉ hi·ªÉu v·ªÅ sinh h·ªçc:

1. **M√¥ t·∫£ c·∫•u tr√∫c**: H√¨nh d·∫°ng, k√≠ch th∆∞·ªõc, th√†nh ph·∫ßn
2. **Gi·∫£i th√≠ch ch·ª©c nƒÉng**: Vai tr√≤ v√† ho·∫°t ƒë·ªông
3. **T√¨m m·ªëi li√™n h·ªá**: Quan h·ªá v·ªõi c√°c b·ªô ph·∫≠n kh√°c
4. **·ª®ng d·ª•ng trong ƒë·ªùi s·ªëng**: T√°c ƒë·ªông ƒë·∫øn con ng∆∞·ªùi v√† m√¥i tr∆∞·ªùng

ü¶ã **Kh√°m ph√°**: Sinh h·ªçc c√≥ m·∫∑t kh·∫Øp n∆°i trong cu·ªôc s·ªëng!
üåü **Khuy·∫øn kh√≠ch**: H√£y y√™u th∆∞∆°ng v√† b·∫£o v·ªá thi√™n nhi√™n!""",

            'vƒÉn': f"""üìö C√¢u h·ªèi vƒÉn h·ªçc: "{question}"

‚úçÔ∏è ƒê·ªÉ ph√¢n t√≠ch vƒÉn h·ªçc:

1. **ƒê·ªçc hi·ªÉu**: N·∫Øm b·∫Øt n·ªôi dung v√† th√¥ng ƒëi·ªáp ch√≠nh
2. **Ph√¢n t√≠ch ngh·ªá thu·∫≠t**: Ng√¥n ng·ªØ, h√¨nh ·∫£nh, bi·ªán ph√°p tu t·ª´
3. **ƒê√°nh gi√° gi√° tr·ªã**: √ù nghƒ©a nh√¢n vƒÉn v√† x√£ h·ªôi
4. **Li√™n h·ªá th·ª±c t·∫ø**: B√†i h·ªçc cho cu·ªôc s·ªëng hi·ªán t·∫°i

üìñ **C·∫£m nh·∫≠n**: VƒÉn h·ªçc gi√∫p ta hi·ªÉu s√¢u h∆°n v·ªÅ con ng∆∞·ªùi v√† cu·ªôc s·ªëng!
üåü **Khuy·∫øn kh√≠ch**: H√£y ƒë·ªçc nhi·ªÅu s√°ch ƒë·ªÉ ph√°t tri·ªÉn t√¢m h·ªìn!""",
        }
        
        return fallback_responses.get(subject, f"""üìñ C√¢u h·ªèi h·ªçc t·∫≠p: "{question}"

üéØ ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y m·ªôt c√°ch t·ªët nh·∫•t:

1. **Nghi√™n c·ª©u t√†i li·ªáu**: T√¨m hi·ªÉu t·ª´ s√°ch gi√°o khoa v√† t√†i li·ªáu tham kh·∫£o
2. **Ph√¢n t√≠ch ƒë·ªÅ b√†i**: X√°c ƒë·ªãnh r√µ y√™u c·∫ßu c·∫ßn tr·∫£ l·ªùi
3. **T·ªï ch·ª©c √Ω t∆∞·ªüng**: S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic
4. **Tr√¨nh b√†y r√µ r√†ng**: S·ª≠ d·ª•ng ng√¥n t·ª´ ch√≠nh x√°c v√† d·ªÖ hi·ªÉu

üí° **G·ª£i √Ω**: H√£y tham kh·∫£o nhi·ªÅu ngu·ªìn t√†i li·ªáu ƒë·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán!

ü§î **L∆∞u √Ω**: T√¥i khuy·∫øn kh√≠ch b·∫°n th·∫£o lu·∫≠n v·ªõi gi√°o vi√™n ho·∫∑c b·∫°n b√® ƒë·ªÉ hi·ªÉu s√¢u h∆°n v·ªÅ v·∫•n ƒë·ªÅ n√†y.

üåü **Khuy·∫øn kh√≠ch**: H·ªçc t·∫≠p l√† h√†nh tr√¨nh su·ªët ƒë·ªùi, h√£y lu√¥n t√≤ m√≤ v√† kh√°m ph√°!""")

class FollowUpGenerator:
    """Generate follow-up questions for deeper learning"""
    
    def __init__(self):
        self.question_patterns = {
            'to√°n': [
                "B·∫°n c√≥ th·ªÉ √°p d·ª•ng ph∆∞∆°ng ph√°p n√†y cho b√†i t·∫≠p kh√°c kh√¥ng?",
                "T·∫°i sao ch√∫ng ta s·ª≠ d·ª•ng c√¥ng th·ª©c n√†y?",
                "C√≥ c√°ch n√†o kh√°c ƒë·ªÉ gi·∫£i b√†i n√†y kh√¥ng?"
            ],
            'l√Ω': [
                "Hi·ªán t∆∞·ª£ng n√†y c√≥ ·ª©ng d·ª•ng g√¨ trong th·ª±c t·∫ø?",
                "ƒêi·ªÅu g√¨ s·∫Ω x·∫£y ra n·∫øu thay ƒë·ªïi ƒëi·ªÅu ki·ªán?",
                "C√≥ th·ªÉ gi·∫£i th√≠ch b·∫±ng ƒë·ªãnh lu·∫≠t n√†o kh√°c?"
            ],
            'h√≥a': [
                "Ph·∫£n ·ª©ng n√†y c√≥ ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn m√¥i tr∆∞·ªùng?",
                "L√†m th·∫ø n√†o ƒë·ªÉ tƒÉng hi·ªáu su·∫•t ph·∫£n ·ª©ng?",
                "C√≥ th·ªÉ d·ª± ƒëo√°n s·∫£n ph·∫©m c·ªßa ph·∫£n ·ª©ng t∆∞∆°ng t·ª±?"
            ],
            'sinh': [
                "C∆° ch·∫ø n√†y ti·∫øn h√≥a nh∆∞ th·∫ø n√†o?",
                "C√≥ s·ª± t∆∞∆°ng ƒë·ªìng n√†o v·ªõi sinh v·∫≠t kh√°c?",
                "ƒêi·ªÅu g√¨ x·∫£y ra khi c√≥ r·ªëi lo·∫°n?"
            ],
            'vƒÉn': [
                "T√°c ph·∫©m n√†y ph·∫£n √°nh ƒëi·ªÅu g√¨ v·ªÅ th·ªùi ƒë·∫°i?",
                "C√≥ th·ªÉ so s√°nh v·ªõi t√°c ph·∫©m n√†o kh√°c?",
                "Th√¥ng ƒëi·ªáp n√†y c√≤n ph√π h·ª£p hi·ªán t·∫°i kh√¥ng?"
            ],
            'kh√°c': [
                "B·∫°n c√≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ ch·ªß ƒë·ªÅ n√†y?",
                "C√≥ mu·ªën t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ v·∫•n ƒë·ªÅ n√†o?",
                "Ph·∫ßn n√†o b·∫°n c·∫£m th·∫•y ch∆∞a r√µ?"
            ]
        }
    
    def generate_follow_ups(self, subject: str, question: str) -> List[str]:
        """Generate relevant follow-up questions"""
        base_questions = self.question_patterns.get(subject, self.question_patterns['kh√°c'])
        
        # Analyze question type for more specific follow-ups
        question_lower = question.lower()
        
        specific_questions = []
        
        if 't·∫°i sao' in question_lower or 'v√¨ sao' in question_lower:
            specific_questions.append("B·∫°n c√≥ mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ nguy√™n nh√¢n s√¢u xa kh√¥ng?")
        
        if 'nh∆∞ th·∫ø n√†o' in question_lower or 'c√°ch n√†o' in question_lower:
            specific_questions.append("C√≥ ph∆∞∆°ng ph√°p n√†o kh√°c ƒë·ªÉ th·ª±c hi·ªán kh√¥ng?")
        
        if 'so s√°nh' in question_lower:
            specific_questions.append("B·∫°n mu·ªën so s√°nh th√™m v·ªõi tr∆∞·ªùng h·ª£p n√†o kh√°c?")
        
        if 'v√≠ d·ª•' in question_lower:
            specific_questions.append("B·∫°n c·∫ßn th√™m v√≠ d·ª• c·ª• th·ªÉ n√†o?")
        
        # Combine and limit
        all_questions = specific_questions + base_questions
        return all_questions[:3]

class StudyChatService:
    """Main study chat service"""
    
    def __init__(self):
        self.subject_detector = SubjectDetector()
        self.llm_assistant = StudyAssistantLLM()
        self.follow_up_generator = FollowUpGenerator()
        self.is_initialized = False
        
        # Conversation memory (simple in-memory cache)
        self.conversation_memory = {}
    
    async def initialize(self) -> bool:
        """Initialize the study chat service"""
        try:
            logger.info("Initializing Study Chat Service...")
            
            # Load LLM model
            llm_loaded = await self.llm_assistant.load_model()
            if not llm_loaded:
                logger.warning("LLM model not loaded, will use fallback responses")
            
            self.is_initialized = True
            logger.info("Study Chat Service initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize study chat service: {e}")
            return False
    
    async def process_message(self, request: StudyChatRequest) -> StudyChatResponse:
        """Process a study chat message"""
        start_time = time.time()
        
        try:
            logger.info(f"Processing study message: {request.message[:50]}...")
            
            # Detect subject if not provided
            subject = request.subject or self.subject_detector.detect_subject(request.message)
            
            # Get conversation context
            conversation_context = []
            if request.conversation_id and request.conversation_id in self.conversation_memory:
                conversation_context = self.conversation_memory[request.conversation_id][-3:]  # Last 3 messages
            
            # Generate response using LLM
            response_text, confidence = await self.llm_assistant.generate_response(
                request.message, 
                subject, 
                request.difficulty or "intermediate"
            )
            
            # Generate follow-up questions
            follow_ups = self.follow_up_generator.generate_follow_ups(subject, request.message)
            
            # Related topics (simple keyword-based)
            related_topics = self._get_related_topics(subject, request.message)
            
            # Store in conversation memory
            if request.conversation_id:
                if request.conversation_id not in self.conversation_memory:
                    self.conversation_memory[request.conversation_id] = []
                
                self.conversation_memory[request.conversation_id].extend([
                    {"role": "user", "content": request.message, "timestamp": datetime.now()},
                    {"role": "assistant", "content": response_text, "timestamp": datetime.now()}
                ])
                
                # Keep only last 10 exchanges (20 messages)
                if len(self.conversation_memory[request.conversation_id]) > 20:
                    self.conversation_memory[request.conversation_id] = self.conversation_memory[request.conversation_id][-20:]
            
            processing_time = time.time() - start_time
            
            return StudyChatResponse(
                response=response_text,
                conversation_id=request.conversation_id,
                subject_detected=subject,
                confidence=confidence,
                follow_up_questions=follow_ups,
                related_topics=related_topics,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error processing study message: {e}")
            return StudyChatResponse(
                response="Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c di·ªÖn ƒë·∫°t c√¢u h·ªèi kh√°c c√°ch.",
                conversation_id=request.conversation_id,
                subject_detected="kh√°c",
                confidence=0.0,
                follow_up_questions=["B·∫°n c√≥ th·ªÉ h·ªèi l·∫°i b·∫±ng c√°ch kh√°c kh√¥ng?"],
                related_topics=[],
                processing_time=time.time() - start_time,
                success=False,
                message=f"Processing error: {str(e)}"
            )
    
    def _get_related_topics(self, subject: str, question: str) -> List[str]:
        """Get related topics based on subject and question"""
        related_topics_map = {
            'to√°n': ['ƒê·∫°i s·ªë', 'H√¨nh h·ªçc', 'Gi·∫£i t√≠ch', 'Th·ªëng k√™', 'L∆∞·ª£ng gi√°c'],
            'l√Ω': ['C∆° h·ªçc', 'ƒêi·ªán h·ªçc', 'Quang h·ªçc', 'Nhi·ªát h·ªçc', 'V·∫≠t l√Ω hi·ªán ƒë·∫°i'],
            'h√≥a': ['H√≥a v√¥ c∆°', 'H√≥a h·ªØu c∆°', 'H√≥a ph√¢n t√≠ch', 'H√≥a l√Ω', 'Sinh h√≥a'],
            'sinh': ['T·∫ø b√†o h·ªçc', 'Di truy·ªÅn h·ªçc', 'Sinh th√°i h·ªçc', 'Ti·∫øn h√≥a', 'Sinh l√Ω'],
            'vƒÉn': ['Th∆°', 'Truy·ªán ng·∫Øn', 'Ti·ªÉu thuy·∫øt', 'K·ªãch', 'VƒÉn h·ªçc d√¢n gian'],
            's·ª≠': ['L·ªãch s·ª≠ Vi·ªát Nam', 'L·ªãch s·ª≠ th·∫ø gi·ªõi', 'Kh·∫£o c·ªï h·ªçc', 'ƒê·ªãa l√Ω l·ªãch s·ª≠'],
            'ƒë·ªãa': ['ƒê·ªãa l√Ω t·ª± nhi√™n', 'ƒê·ªãa l√Ω kinh t·∫ø', 'Kh√≠ h·∫≠u h·ªçc', 'ƒê·ªãa ch·∫•t'],
            'anh': ['Ng·ªØ ph√°p', 'T·ª´ v·ª±ng', 'Ph√°t √¢m', 'Giao ti·∫øp', 'Vi·∫øt lu·∫≠n'],
            'tin': ['L·∫≠p tr√¨nh', 'C∆° s·ªü d·ªØ li·ªáu', 'M·∫°ng m√°y t√≠nh', 'Tr√≠ tu·ªá nh√¢n t·∫°o'],
            'kh√°c': ['H·ªçc t·∫≠p hi·ªáu qu·∫£', 'Ph∆∞∆°ng ph√°p nghi√™n c·ª©u', 'K·ªπ nƒÉng thi c·ª≠']
        }
        
        base_topics = related_topics_map.get(subject, related_topics_map['kh√°c'])
        
        # Filter based on question content (simple keyword matching)
        question_lower = question.lower()
        relevant_topics = []
        
        for topic in base_topics:
            topic_lower = topic.lower()
            # Simple relevance check
            if any(word in question_lower for word in topic_lower.split()):
                relevant_topics.append(topic)
        
        # If no specific matches, return first 3 topics
        if not relevant_topics:
            relevant_topics = base_topics[:3]
        
        return relevant_topics[:3]

# Global service instance
study_service = StudyChatService()